{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: 2024-11-25\n",
    "title: \"Using an LLM for information extraction\"\n",
    "keywords: [\n",
    "    \"Enron email dataset\",\n",
    "    \"Information extraction\",\n",
    "    \"LLM\",\n",
    "    \"NLP\",\n",
    "    \"llama.cpp\"\n",
    "]\n",
    "license: \"CC BY\"\n",
    "execute: \n",
    "  cache: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will see how to use a local LLM to extract structured information from emails.\n",
    "\n",
    "My very first project when I started working at appliedAI Initiative in 2021, involved information extraction from emails for a company that makes a document management system. Back then LLMs were not yet as widespread and as useful as they are right now, so we decided to train a model from scratch. We however didn't have any labelled data for training because we couldn't use their customer data due to privacy reasons and had to resort to manually labelling emails from the [Enron email dataset]() and in the end the results were not very impressive.\n",
    "\n",
    "Now, this type of application is simpler than ever and I want to demonstrate that in this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from email.parser import Parser\n",
    "from functools import partial\n",
    "from statistics import fmean\n",
    "from typing import Any, Callable, Literal\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from llama_cpp import Llama\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "random.seed(16)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email information model\n",
    "\n",
    "We start by defining the model and by extension the schema of the structured information we want to extract.\n",
    "\n",
    "For that we use [Pydantic](), to define the email information, sender and recipient models along with methods to compare and compute a similarity score for each model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailBaseModel(BaseModel, extra=\"forbid\"):\n",
    "    \"\"\"Base model class for email-related information extraction.\n",
    "\n",
    "    This class extends BaseModel and provides common functionality for comparing\n",
    "    string attributes between email information objects.\n",
    "\n",
    "    Note:\n",
    "        The extra=\"forbid\" parameter ensures no additional attributes can be added\n",
    "        beyond those explicitly defined.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_strings(a: str | None, b: str | None) -> float:\n",
    "        \"\"\"Computes similarity ratio between two possibly None strings.\n",
    "\n",
    "        Uses SequenceMatcher to calculate string similarity when both inputs are\n",
    "        not missing (None). Handles cases where one or both inputs are None.\n",
    "\n",
    "        Args:\n",
    "            a: First string to compare, or None\n",
    "            b: Second string to compare, or None\n",
    "\n",
    "        Returns:\n",
    "            Similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical strings or both are None\n",
    "                - 0.0 indicates completely different strings or one of them is None\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity\n",
    "        \"\"\"\n",
    "        if a is None and b is None:\n",
    "            similarity = 1.0\n",
    "        elif a is not None and b is not None:\n",
    "            similarity = SequenceMatcher(None, a, b).ratio()\n",
    "        elif a is not None:\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(EmailBaseModel):\n",
    "    \"\"\"Represents a sender of an email with their associated information.\n",
    "\n",
    "    Stores and compares sender details including name, email, phone number,\n",
    "    role, and organization.\n",
    "\n",
    "    Attributes:\n",
    "        name: The sender's full name if available\n",
    "        email: The sender's email address\n",
    "        phone_number: The sender's phone number if available\n",
    "        role: The sender's professional role if available\n",
    "        organization: The sender's organization if available\n",
    "    \"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    email: str\n",
    "    phone_number: str | None = None\n",
    "    role: str | None = None\n",
    "    organization: str | None = None\n",
    "\n",
    "    def compare(self, other: \"Sender\") -> float:\n",
    "        \"\"\"Compares this sender with another sender object.\n",
    "\n",
    "        Calculates similarity by comparing all attributes using string comparison\n",
    "        and returns the mean similarity across all fields.\n",
    "\n",
    "        Args:\n",
    "            other: Another Sender object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical senders\n",
    "                - 0.0 indicates completely different senders or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across fields\n",
    "\n",
    "        Note:\n",
    "            Returns 0.0 if other is not a Sender instance.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Sender):\n",
    "            return 0.0\n",
    "\n",
    "        name_similarity = self._compare_strings(self.name, other.name)\n",
    "        email_similarity = self._compare_strings(self.email, other.email)\n",
    "        phone_number_similarity = self._compare_strings(\n",
    "            self.phone_number, other.phone_number\n",
    "        )\n",
    "        role_similarity = self._compare_strings(self.role, other.role)\n",
    "        organization_similarity = self._compare_strings(\n",
    "            self.organization, other.organization\n",
    "        )\n",
    "        return fmean(\n",
    "            [\n",
    "                name_similarity,\n",
    "                email_similarity,\n",
    "                phone_number_similarity,\n",
    "                role_similarity,\n",
    "                organization_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipient(EmailBaseModel):\n",
    "    \"\"\"Represents a recipient of an email with their associated information.\n",
    "\n",
    "    Stores and compares recipient details including name, email, phone number,\n",
    "    role, organization, and their type of recipiency (to, cc, bcc).\n",
    "\n",
    "    Attributes:\n",
    "        name: The recipient's full name if available\n",
    "        email: The recipient's email address\n",
    "        phone_number: The recipient's phone number if available\n",
    "        role: The recipient's professional role if available\n",
    "        organization: The recipient's organization if available\n",
    "        type: The type of recipient (\"to\", \"cc\", or \"bcc\")\n",
    "    \"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    email: str\n",
    "    phone_number: str | None = None\n",
    "    role: str | None = None\n",
    "    organization: str | None = None\n",
    "    type: Literal[\"to\", \"cc\", \"bcc\"] = \"to\"\n",
    "\n",
    "    def compare(self, other: \"Recipient\") -> float:\n",
    "        \"\"\"Compares this recipient with another recipient object.\n",
    "\n",
    "        Calculates similarity by comparing all attributes using string comparison\n",
    "        and includes exact matching for recipient type. Returns the mean\n",
    "        similarity across all fields.\n",
    "\n",
    "        Args:\n",
    "            other: Another Recipient object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical recipients\n",
    "                - 0.0 indicates completely different recipients or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across fields\n",
    "\n",
    "        Note:\n",
    "            Returns 0.0 if other is not a Recipient instance.\n",
    "            Recipient type comparison is binary: 1.0 if identical, 0.0 if different.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Recipient):\n",
    "            return 0.0\n",
    "\n",
    "        name_similarity = self._compare_strings(self.name, other.name)\n",
    "        email_similarity = self._compare_strings(self.email, other.email)\n",
    "        phone_number_similarity = self._compare_strings(\n",
    "            self.phone_number, other.phone_number\n",
    "        )\n",
    "        role_similarity = self._compare_strings(self.role, other.role)\n",
    "        organization_similarity = self._compare_strings(\n",
    "            self.organization, other.organization\n",
    "        )\n",
    "        type_similarity = 1.0 if self.type == other.type else 0.0\n",
    "        return fmean(\n",
    "            [\n",
    "                name_similarity,\n",
    "                email_similarity,\n",
    "                phone_number_similarity,\n",
    "                role_similarity,\n",
    "                organization_similarity,\n",
    "                type_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailInformation(EmailBaseModel):\n",
    "    \"\"\"Represents comprehensive information extracted from an email.\n",
    "\n",
    "    Stores and compares email metadata including date, subject, sender information,\n",
    "    and a list of recipients. Provides functionality to compare two email information\n",
    "    objects for similarity.\n",
    "\n",
    "    Attributes:\n",
    "        date: The date of the email\n",
    "        subject: The email subject line\n",
    "        sender: Sender object containing sender information\n",
    "        recipients: List of Recipient objects containing recipient information\n",
    "    \"\"\"\n",
    "\n",
    "    date: str\n",
    "    subject: str\n",
    "    sender: Sender\n",
    "    recipients: list[Recipient]\n",
    "\n",
    "    def compare(self, other: \"EmailInformation\") -> float:\n",
    "        \"\"\"Compares this email information with another email information object.\n",
    "\n",
    "        Performs a detailed comparison of all email attributes including sender\n",
    "        and recipient information. For recipients, finds the best matching recipient\n",
    "        pairs between the two emails and averages their similarities.\n",
    "\n",
    "        Args:\n",
    "            other: Another EmailInformation object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical email information\n",
    "                - 0.0 indicates completely different emails or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across all fields\n",
    "\n",
    "        Note:\n",
    "            - Returns 0.0 if other is not an EmailInformation instance.\n",
    "            - Returns 1.0 if self == other (exact match).\n",
    "            - Recipient comparison finds the best matching recipient for each\n",
    "              recipient in self.recipients among other.recipients.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, EmailInformation):\n",
    "            return 0.0\n",
    "        if self == other:\n",
    "            return 1.0\n",
    "        date_similarity = self._compare_strings(self.date, other.date)\n",
    "        subject_similarity = self._compare_strings(self.subject, other.subject)\n",
    "        sender_similarity = self.sender.compare(other.sender)\n",
    "\n",
    "        if self.recipients == other.recipients:\n",
    "            recipient_similarity = 1.0\n",
    "        else:\n",
    "            recipient_similarities = []\n",
    "            for recipient_1 in self.recipients:\n",
    "                recipient_1_similarity = 0.0\n",
    "                for recipient_2 in other.recipients:\n",
    "                    recipient_1_similarity = max(\n",
    "                        recipient_1_similarity, recipient_1.compare(recipient_2)\n",
    "                    )\n",
    "                recipient_similarities.append(recipient_1_similarity)\n",
    "            if recipient_similarities:\n",
    "                recipient_similarity = fmean(recipient_similarities)\n",
    "            else:\n",
    "                recipient_similarity = 0.0\n",
    "\n",
    "        return fmean(\n",
    "            [\n",
    "                date_similarity,\n",
    "                subject_similarity,\n",
    "                sender_similarity,\n",
    "                recipient_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our different approaches, we also define a helper fuction to run the extraction approach over all emails in a given dataset and compute the accuracy (similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction(\n",
    "    extract_fn: Callable[[str], EmailInformation],\n",
    "    dataset: list[dict[str, Any]],\n",
    ") -> list[float]:\n",
    "    \"\"\"Evaluates an email information extraction function against a ground truth dataset.\n",
    "\n",
    "    Processes each email in the dataset using the provided extraction function and\n",
    "    compares the results against ground truth annotations using the EmailInformation\n",
    "    comparison logic.\n",
    "\n",
    "    Args:\n",
    "        extract_fn: Function that takes a raw email string as input and returns\n",
    "            an EmailInformation object containing the extracted information.\n",
    "        dataset: List of dictionaries, where each dictionary contains:\n",
    "            - 'raw_email': The raw email text to process\n",
    "            - 'extracted_information': Ground truth EmailInformation object\n",
    "\n",
    "    Returns:\n",
    "        List of accuracy scores between 0.0 and 1.0 for each email, where:\n",
    "            - 1.0 indicates perfect extraction matching ground truth\n",
    "            - 0.0 indicates completely incorrect extraction\n",
    "            - Values between indicate partial matching of extracted information\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"Emails\", leave=False):\n",
    "        extracted_information = extract_fn(sample[\"raw_email\"])\n",
    "\n",
    "        accuracy = sample[\"extracted_information\"].compare(extracted_information)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Similarly to my first project, we will use as data emails from the [Enron dataset](https://www.cs.cmu.edu/~enron/). \n",
    "\n",
    "I went ahead and created a sample of 20 emails and manually extracted information from them in order to be able to evaluate the different methods.\n",
    "\n",
    "We load the dataset and split it into train and test sets with with split sizes 0.4, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"enron_emails.yml\") as f:\n",
    "    all_email_data = yaml.safe_load(f)\n",
    "\n",
    "for email_data in all_email_data:\n",
    "    email_data[\"extracted_information\"] = EmailInformation.model_validate(\n",
    "        email_data[\"extracted_information\"]\n",
    "    )\n",
    "\n",
    "all_indices = list(range(len(all_email_data)))\n",
    "train_set_indices = random.choices(all_indices, k=int(0.4 * len(all_email_data)))\n",
    "test_set_indices = list(set(all_indices).difference(train_set_indices))\n",
    "train_set = [all_email_data[i] for i in train_set_indices]\n",
    "test_set = [all_email_data[i] for i in test_set_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_email = train_set[0]\n",
    "sample_raw_email = sample_test_email[\"raw_email\"]\n",
    "sample_email_information = sample_test_email[\"extracted_information\"]\n",
    "print(f\"Sample raw email:\\n\\n{sample_raw_email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Sample ground truth extracted information:\\n\\n{sample_email_information.model_dump_json(indent=4)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach - Use Python's builtin email parser\n",
    "\n",
    "As a first approach, we will simply use Python's builtin email parser from the [email](https://docs.python.org/3/library/email.examples.html) package.\n",
    "\n",
    "We define an extraction function that parses the emails and extracts information from them without much validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_with_builtin_parser(raw_email: str) -> EmailInformation:\n",
    "    \"\"\"Extracts structured information from a raw email using Python's built-in email parser.\n",
    "\n",
    "    Parses the raw email text to extract metadata including date, subject, sender, and recipients.\n",
    "    Handles special X-headers for additional information like sender and recipient names.\n",
    "\n",
    "    Args:\n",
    "        raw_email: Raw email text including headers and body.\n",
    "\n",
    "    Returns:\n",
    "        Structured object containing the extracted information with:\n",
    "            - date: Formatted as DD.MM.YYYY\n",
    "            - subject: Email subject line\n",
    "            - sender: Sender information including email and optional name\n",
    "            - recipients: List of recipients (to/cc/bcc) with email and optional name,\n",
    "                sorted by email address\n",
    "    \"\"\"\n",
    "    parser = Parser()\n",
    "    email = parser.parsestr(raw_email)\n",
    "    parsed_date = datetime.strptime(\n",
    "        email[\"date\"].strip().split(\"(\")[0], \"%a, %d %b %Y %H:%M:%S %z \"\n",
    "    )\n",
    "    formatted_date = parsed_date.strftime(\"%d.%m.%Y\")\n",
    "    email_dict = {\"date\": formatted_date, \"subject\": email[\"subject\"].strip()}\n",
    "    sender = {\"email\": email[\"from\"].strip()}\n",
    "    if email[\"X-from\"] and email[\"X-from\"].strip() != email[\"from\"]:\n",
    "        sender[\"name\"] = email[\"X-from\"].strip()\n",
    "    email_dict[\"sender\"] = sender\n",
    "\n",
    "    recipients = []\n",
    "    for type_ in [\"to\", \"cc\", \"bcc\"]:\n",
    "        recipient_names = email.get(f\"X-{type_}\", \"\").strip().split(\",\")\n",
    "        recipient_emails = email.get(type_, None)\n",
    "        if recipient_emails is None:\n",
    "            continue\n",
    "        recipient_emails = recipient_emails.split(\",\")\n",
    "        if len(recipient_emails) != len(recipient_names):\n",
    "            recipient_names = [\"\"] * len(recipient_emails)\n",
    "        for recipient_name, recipient_email in zip(recipient_names, recipient_emails):\n",
    "            recipient = {\"type\": type_, \"email\": recipient_email.strip()}\n",
    "            if recipient_name and recipient_name != recipient_email:\n",
    "                recipient[\"name\"] = recipient_name.strip()\n",
    "            recipients.append(recipient)\n",
    "\n",
    "    email_dict[\"recipients\"] = list(sorted(recipients, key=lambda x: x[\"email\"]))\n",
    "\n",
    "    return EmailInformation.model_validate(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information = extract_information_with_builtin_parser(sample_raw_email)\n",
    "sample_accuracy = sample_email_information.compare(extracted_information)\n",
    "print(\n",
    "    f\"Sample extracted information with builtin parser:\\n\\n{extracted_information.model_dump_json(indent=4)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample email information extraction accuracy for builtin parser: {sample_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_test_accuracies = evaluate_extraction(\n",
    "    extract_information_with_builtin_parser, test_set\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy for builtin parser: {np.mean(builtin_test_accuracies) * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=builtin_test_accuracies);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach - Use LLM zero-shot extraction with JSON schema\n",
    "\n",
    "We will use [llama-cpp-python](), a python wrapper for [llama.cpp](), to run an LLM locally.\n",
    "\n",
    "It has support for passing a json schema to enforce structured output generation without having to play around with the prompt and retrying in case of failed json generation.\n",
    "\n",
    "> **Note**: There [known performance issues](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md#troubleshooting) with llama.cpp's structured output generation using grammars and, by extension, json schemas especially with nested objects.\n",
    "\n",
    "We use a quantized version of Llama 3.2 3B Instruct as LLM and limit the context length to 16384 in order to handle long raw emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    \"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-3B-Instruct-Q8_0.gguf\",\n",
    "    n_ctx=16384,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a system prompt with instructions for the LLM. Inspired by [this blog post](https://www.boundaryml.com/blog/type-definition-prompting-baml), I decided to use a JSON type definition in the prompt instead of a JSON schema because it is shorter, produced better results and is more human-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n",
    "\n",
    "{\n",
    "    \"date\": string,\n",
    "    \"subject\": string,\n",
    "    \"sender\": {\n",
    "        \"name\": string | null,\n",
    "        \"email\": string,\n",
    "        \"phone_number\": string | null,\n",
    "        \"role\": string | null,\n",
    "        \"organization\": string | null\n",
    "    },\n",
    "    \"recipients\": {\n",
    "        \"name\": string | null,\n",
    "        \"email\": string,\n",
    "        \"phone_number\": string | null,\n",
    "        \"role\": string | null,\n",
    "        \"organization\": string | null,\n",
    "        \"to\": enum([\"to\", \"cc\", \"bcc\"])\n",
    "    }[]\n",
    "}\n",
    "\"\"\"\n",
    "print(f\"System prompt:\\n---\\n\\n{system_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_with_llm(\n",
    "    raw_email: str, *, system_prompt: str\n",
    ") -> EmailInformation:\n",
    "    \"\"\"Extracts structured information from a raw email using an LLM.\n",
    "\n",
    "    Uses chat completion API to parse email content into structured format.\n",
    "    Enforces output schema validation using EmailInformation model specification.\n",
    "\n",
    "    Args:\n",
    "        raw_email: Raw email text including headers and body.\n",
    "        system_prompt: System prompt for the LLM that defines the extraction task.\n",
    "\n",
    "    Returns:\n",
    "        Structured object containing the extracted information, validated against the EmailInformation schema.\n",
    "    \"\"\"\n",
    "    response_format = {\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": EmailInformation.model_json_schema(),\n",
    "    }\n",
    "    output = llm.create_chat_completion_openai_v1(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": raw_email},\n",
    "        ],\n",
    "        response_format=response_format,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    extracted_information = EmailInformation.model_validate_json(\n",
    "        output.choices[0].message.content\n",
    "    )\n",
    "    return extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information = extract_information_with_llm(\n",
    "    sample_raw_email, system_prompt=system_prompt\n",
    ")\n",
    "sample_accuracy = sample_email_information.compare(extracted_information)\n",
    "print(\n",
    "    f\"Sample extracted information with llm zero-shot:\\n\\n{extracted_information.model_dump_json(indent=4)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample email information extraction accuracy for llm zero-shot: {sample_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_test_accuracies = evaluate_extraction(\n",
    "    partial(extract_information_with_llm, system_prompt=system_prompt), test_set\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy: {np.mean(llm_test_accuracies) * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=llm_test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third approach - LLM few-shot extraction with JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "\n",
    "for i in trange(len(train_set), desc=\"Example\"):\n",
    "    example = train_set[i]\n",
    "    train_set_without_example = train_set[:i] + train_set[i + 1 :]\n",
    "    system_prompt_with_example = (\n",
    "        system_prompt\n",
    "        + f\"\"\"\n",
    "Use the following example of raw email and extracted information as reference:\n",
    "\n",
    "# Raw email\n",
    "\n",
    "{example[\"raw_email\"]}\n",
    "\n",
    "# Extracted information\n",
    "\n",
    "{example[\"extracted_information\"].model_dump_json(indent=2)}\n",
    "\"\"\"\n",
    "    )\n",
    "    accuracies = evaluate_extraction(\n",
    "        partial(extract_information_with_llm, system_prompt=system_prompt_with_example),\n",
    "        train_set_without_example,\n",
    "    )\n",
    "    mean_accuracy = np.mean(accuracies).item()\n",
    "    train_accuracies.append((mean_accuracy, system_prompt_with_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=[x[0] for x in train_accuracies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmin([x[0] for x in train_accuracies])\n",
    "best_system_prompt_with_example = train_accuracies[best_index][1]\n",
    "print(best_system_prompt_with_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information = extract_information_with_llm(\n",
    "    sample_raw_email, system_prompt=best_system_prompt_with_example\n",
    ")\n",
    "sample_accuracy = sample_email_information.compare(extracted_information)\n",
    "print(\n",
    "    f\"Sample extracted information with llm zero-shot:\\n\\n{extracted_information.model_dump_json(indent=4)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample email information extraction accuracy for llm zero-shot: {sample_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_llm_test_accuracies = evaluate_extraction(\n",
    "    partial(\n",
    "        extract_information_with_llm, system_prompt=best_system_prompt_with_example\n",
    "    ),\n",
    "    test_set,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy: {np.mean(few_shot_llm_test_accuracies) * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=few_shot_llm_test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Builtin\": builtin_test_accuracies,\n",
    "        \"LLM Zero-Shot\": llm_test_accuracies,\n",
    "        \"LLM Few-Shot\": few_shot_llm_test_accuracies,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this post, we have ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_information_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
