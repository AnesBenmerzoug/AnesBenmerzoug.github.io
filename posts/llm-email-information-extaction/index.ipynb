{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: 2024-12-09\n",
    "title: \"Using an LLM for information extraction\"\n",
    "keywords: [\n",
    "    \"Enron email dataset\",\n",
    "    \"Information extraction\",\n",
    "    \"LLM\",\n",
    "    \"NLP\",\n",
    "    \"llama.cpp\",\n",
    "    \"Pydantic\"\n",
    "]\n",
    "license: \"CC BY\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    code-overflow: wrap\n",
    "execute: \n",
    "  cache: true\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will see how to use a local LLM to extract structured information from emails.\n",
    "\n",
    "My very first project when I started working at appliedAI Initiative in 2021, involved information extraction from emails for a company that makes a document management system. Back then LLMs were not yet as widespread and as useful as they are right now, so we decided to train a model from scratch. We however didn't have any labelled data for training because we couldn't use their customer data due to privacy reasons and had to resort to manually labelling emails from the [Enron email dataset]() and in the end the results were not very impressive.\n",
    "\n",
    "Now, this type of application is simpler than ever and I want to demonstrate that in this blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from email.parser import Parser\n",
    "from functools import partial\n",
    "from statistics import fmean\n",
    "from typing import Any, Callable, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from llama_cpp import Llama\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "random.seed(16)\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email information model\n",
    "\n",
    "We start by defining the model and by extension the schema of the structured information we want to extract.\n",
    "\n",
    "For that we use [Pydantic](), to define the email information, sender and recipient models along with methods to compare and compute a similarity score for each model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**EmailBaseModel**: Base model class for email-related information extraction.\"\n",
    "\n",
    "\n",
    "class EmailBaseModel(BaseModel, extra=\"forbid\"):\n",
    "    \"\"\"Base model class for email-related information extraction.\n",
    "\n",
    "    This class extends BaseModel and provides common functionality for comparing\n",
    "    string attributes between email information objects.\n",
    "\n",
    "    Note:\n",
    "        The extra=\"forbid\" parameter ensures no additional attributes can be added\n",
    "        beyond those explicitly defined.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_strings(a: str | None, b: str | None) -> float:\n",
    "        \"\"\"Computes similarity ratio between two possibly None strings.\n",
    "\n",
    "        Uses SequenceMatcher to calculate string similarity when both inputs are\n",
    "        not missing (None). Handles cases where one or both inputs are None.\n",
    "\n",
    "        Args:\n",
    "            a: First string to compare, or None\n",
    "            b: Second string to compare, or None\n",
    "\n",
    "        Returns:\n",
    "            Similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical strings or both are None\n",
    "                - 0.0 indicates completely different strings or one of them is None\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity\n",
    "        \"\"\"\n",
    "        if a is None and b is None:\n",
    "            similarity = 1.0\n",
    "        elif a is not None and b is not None:\n",
    "            similarity = SequenceMatcher(None, a, b).ratio()\n",
    "        elif a is not None:\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**Sender**: Represents a sender of an email with their associated information.\"\n",
    "\n",
    "\n",
    "class Sender(EmailBaseModel):\n",
    "    \"\"\"Represents a sender of an email with their associated information.\n",
    "\n",
    "    Stores and compares sender details including name, email, phone number,\n",
    "    role, and organization.\n",
    "\n",
    "    Attributes:\n",
    "        name: The sender's full name if available\n",
    "        email: The sender's email address\n",
    "        phone_number: The sender's phone number if available\n",
    "        role: The sender's professional role if available\n",
    "        organization: The sender's organization if available\n",
    "    \"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    email: str\n",
    "    phone_number: str | None = None\n",
    "    role: str | None = None\n",
    "    organization: str | None = None\n",
    "\n",
    "    def compare(self, other: \"Sender\") -> float:\n",
    "        \"\"\"Compares this sender with another sender object.\n",
    "\n",
    "        Calculates similarity by comparing all attributes using string comparison\n",
    "        and returns the mean similarity across all fields.\n",
    "\n",
    "        Args:\n",
    "            other: Another Sender object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical senders\n",
    "                - 0.0 indicates completely different senders or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across fields\n",
    "\n",
    "        Note:\n",
    "            Returns 0.0 if other is not a Sender instance.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Sender):\n",
    "            return 0.0\n",
    "\n",
    "        name_similarity = self._compare_strings(self.name, other.name)\n",
    "        email_similarity = self._compare_strings(self.email, other.email)\n",
    "        phone_number_similarity = self._compare_strings(\n",
    "            self.phone_number, other.phone_number\n",
    "        )\n",
    "        role_similarity = self._compare_strings(self.role, other.role)\n",
    "        organization_similarity = self._compare_strings(\n",
    "            self.organization, other.organization\n",
    "        )\n",
    "        return fmean(\n",
    "            [\n",
    "                name_similarity,\n",
    "                email_similarity,\n",
    "                phone_number_similarity,\n",
    "                role_similarity,\n",
    "                organization_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**Recipient**: Represents a recipient of an email with their associated information.\"\n",
    "\n",
    "\n",
    "class Recipient(EmailBaseModel):\n",
    "    \"\"\"Represents a recipient of an email with their associated information.\n",
    "\n",
    "    Stores and compares recipient details including name, email, phone number,\n",
    "    role, organization, and their type of recipiency (to, cc, bcc).\n",
    "\n",
    "    Attributes:\n",
    "        name: The recipient's full name if available\n",
    "        email: The recipient's email address\n",
    "        phone_number: The recipient's phone number if available\n",
    "        role: The recipient's professional role if available\n",
    "        organization: The recipient's organization if available\n",
    "        type: The type of recipient (\"to\", \"cc\", or \"bcc\")\n",
    "    \"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    email: str\n",
    "    phone_number: str | None = None\n",
    "    role: str | None = None\n",
    "    organization: str | None = None\n",
    "    type: Literal[\"to\", \"cc\", \"bcc\"] = \"to\"\n",
    "\n",
    "    def compare(self, other: \"Recipient\") -> float:\n",
    "        \"\"\"Compares this recipient with another recipient object.\n",
    "\n",
    "        Calculates similarity by comparing all attributes using string comparison\n",
    "        and includes exact matching for recipient type. Returns the mean\n",
    "        similarity across all fields.\n",
    "\n",
    "        Args:\n",
    "            other: Another Recipient object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical recipients\n",
    "                - 0.0 indicates completely different recipients or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across fields\n",
    "\n",
    "        Note:\n",
    "            Returns 0.0 if other is not a Recipient instance.\n",
    "            Recipient type comparison is binary: 1.0 if identical, 0.0 if different.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Recipient):\n",
    "            return 0.0\n",
    "\n",
    "        name_similarity = self._compare_strings(self.name, other.name)\n",
    "        email_similarity = self._compare_strings(self.email, other.email)\n",
    "        phone_number_similarity = self._compare_strings(\n",
    "            self.phone_number, other.phone_number\n",
    "        )\n",
    "        role_similarity = self._compare_strings(self.role, other.role)\n",
    "        organization_similarity = self._compare_strings(\n",
    "            self.organization, other.organization\n",
    "        )\n",
    "        type_similarity = 1.0 if self.type == other.type else 0.0\n",
    "        return fmean(\n",
    "            [\n",
    "                name_similarity,\n",
    "                email_similarity,\n",
    "                phone_number_similarity,\n",
    "                role_similarity,\n",
    "                organization_similarity,\n",
    "                type_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**EmailInformation**: Represents information extracted from an email.\"\n",
    "\n",
    "\n",
    "class EmailInformation(EmailBaseModel):\n",
    "    \"\"\"Represents information extracted from an email.\n",
    "\n",
    "    Stores and compares email metadata including date, subject, sender information,\n",
    "    and a list of recipients. Provides functionality to compare two email information\n",
    "    objects for similarity.\n",
    "\n",
    "    Attributes:\n",
    "        date: The date of the email\n",
    "        subject: The email subject line\n",
    "        sender: Sender object containing sender information\n",
    "        recipients: List of Recipient objects containing recipient information\n",
    "    \"\"\"\n",
    "\n",
    "    date: str\n",
    "    subject: str\n",
    "    sender: Sender\n",
    "    recipients: list[Recipient]\n",
    "\n",
    "    def compare(self, other: \"EmailInformation\") -> float:\n",
    "        \"\"\"Compares this email information with another email information object.\n",
    "\n",
    "        Performs a detailed comparison of all email attributes including sender\n",
    "        and recipient information. For recipients, finds the best matching recipient\n",
    "        pairs between the two emails and averages their similarities.\n",
    "\n",
    "        Args:\n",
    "            other: Another EmailInformation object to compare against\n",
    "\n",
    "        Returns:\n",
    "            Mean similarity ratio between 0.0 and 1.0, where:\n",
    "                - 1.0 indicates identical email information\n",
    "                - 0.0 indicates completely different emails or invalid comparison\n",
    "                - Values between 0.0 and 1.0 indicate partial similarity across all fields\n",
    "\n",
    "        Note:\n",
    "            - Returns 0.0 if other is not an EmailInformation instance.\n",
    "            - Returns 1.0 if self == other (exact match).\n",
    "            - Recipient comparison finds the best matching recipient for each\n",
    "              recipient in self.recipients among other.recipients.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, EmailInformation):\n",
    "            return 0.0\n",
    "        if self == other:\n",
    "            return 1.0\n",
    "        date_similarity = self._compare_strings(self.date, other.date)\n",
    "        subject_similarity = self._compare_strings(self.subject, other.subject)\n",
    "        sender_similarity = self.sender.compare(other.sender)\n",
    "\n",
    "        if self.recipients == other.recipients:\n",
    "            recipient_similarity = 1.0\n",
    "        else:\n",
    "            recipient_similarities = []\n",
    "            for recipient_1 in self.recipients:\n",
    "                recipient_1_similarity = 0.0\n",
    "                for recipient_2 in other.recipients:\n",
    "                    recipient_1_similarity = max(\n",
    "                        recipient_1_similarity, recipient_1.compare(recipient_2)\n",
    "                    )\n",
    "                recipient_similarities.append(recipient_1_similarity)\n",
    "            if recipient_similarities:\n",
    "                recipient_similarity = fmean(recipient_similarities)\n",
    "            else:\n",
    "                recipient_similarity = 0.0\n",
    "\n",
    "        return fmean(\n",
    "            [\n",
    "                date_similarity,\n",
    "                subject_similarity,\n",
    "                sender_similarity,\n",
    "                recipient_similarity,\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our different approaches, we also define a helper fuction to run the extraction approach over all emails in a given dataset and compute the accuracy (similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**evaluate_extraction**: Function that evaluates an email information extraction function against a ground truth dataset.\"\n",
    "\n",
    "\n",
    "def evaluate_extraction(\n",
    "    extract_fn: Callable[[str], EmailInformation],\n",
    "    dataset: list[dict[str, Any]],\n",
    ") -> list[float]:\n",
    "    \"\"\"Evaluates an email information extraction function against a ground truth dataset.\n",
    "\n",
    "    Processes each email in the dataset using the provided extraction function and\n",
    "    compares the results against ground truth annotations using the EmailInformation\n",
    "    comparison logic.\n",
    "\n",
    "    Args:\n",
    "        extract_fn: Function that takes a raw email string as input and returns\n",
    "            an EmailInformation object containing the extracted information.\n",
    "        dataset: List of dictionaries, where each dictionary contains:\n",
    "            - 'raw_email': The raw email text to process\n",
    "            - 'extracted_information': Ground truth EmailInformation object\n",
    "\n",
    "    Returns:\n",
    "        List of accuracy scores between 0.0 and 1.0 for each email, where:\n",
    "            - 1.0 indicates perfect extraction matching ground truth\n",
    "            - 0.0 indicates completely incorrect extraction\n",
    "            - Values between indicate partial matching of extracted information\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"Emails\", leave=False):\n",
    "        extracted_information = extract_fn(sample[\"raw_email\"])\n",
    "\n",
    "        accuracy = sample[\"extracted_information\"].compare(extracted_information)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Similarly to my first project, we will use as data emails from the [Enron dataset](https://www.cs.cmu.edu/~enron/). \n",
    "\n",
    "I went ahead and created a sample of 20 emails and manually extracted information from them in order to be able to evaluate the different methods.\n",
    "\n",
    "We load the dataset and split it into train and test sets with with split sizes 4 and 16, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "with open(\"enron_emails.yml\") as f:\n",
    "    all_email_data = yaml.safe_load(f)\n",
    "\n",
    "for email_data in all_email_data:\n",
    "    email_data[\"extracted_information\"] = EmailInformation.model_validate(\n",
    "        email_data[\"extracted_information\"]\n",
    "    )\n",
    "\n",
    "all_indices = list(range(len(all_email_data)))\n",
    "train_set_indices = random.choices(all_indices, k=int(0.2 * len(all_email_data)))\n",
    "test_set_indices = list(set(all_indices).difference(train_set_indices))\n",
    "train_set = [all_email_data[i] for i in train_set_indices]\n",
    "test_set = [all_email_data[i] for i in test_set_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "sample_test_email = train_set[1]\n",
    "sample_raw_email = sample_test_email[\"raw_email\"]\n",
    "sample_email_information = sample_test_email[\"extracted_information\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"Sample raw email:\n",
    "\n",
    "```yaml\n",
    "{sample_raw_email}\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"Sample ground truth extracted information:\n",
    "\n",
    "```json\n",
    "{sample_email_information.model_dump_json(indent=4)}\"\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Use Python's builtin email parser\n",
    "\n",
    "As a baseline approach, we will use Python's builtin email parser from the [email](https://docs.python.org/3/library/email.examples.html) package.\n",
    "\n",
    "We define an extraction function that parses the emails and extracts information from them without much validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**extract_information_with_builtin_parser**: Function that extracts information using Python's built-in email parser.\"\n",
    "\n",
    "\n",
    "def extract_information_with_builtin_parser(raw_email: str) -> EmailInformation:\n",
    "    \"\"\"Extracts structured information from a raw email using Python's built-in email parser.\n",
    "\n",
    "    Parses the raw email text to extract metadata including date, subject, sender, and recipients.\n",
    "    Handles special X-headers for additional information like sender and recipient names.\n",
    "\n",
    "    Args:\n",
    "        raw_email: Raw email text including headers and body.\n",
    "\n",
    "    Returns:\n",
    "        Structured object containing the extracted information with:\n",
    "            - date: Formatted as DD.MM.YYYY\n",
    "            - subject: Email subject line\n",
    "            - sender: Sender information including email and optional name\n",
    "            - recipients: List of recipients (to/cc/bcc) with email and optional name,\n",
    "                sorted by email address\n",
    "    \"\"\"\n",
    "    parser = Parser()\n",
    "    email = parser.parsestr(raw_email)\n",
    "    parsed_date = datetime.strptime(\n",
    "        email[\"date\"].strip().split(\"(\")[0], \"%a, %d %b %Y %H:%M:%S %z \"\n",
    "    )\n",
    "    formatted_date = parsed_date.strftime(\"%d.%m.%Y\")\n",
    "    email_dict = {\"date\": formatted_date, \"subject\": email[\"subject\"].strip()}\n",
    "    sender = {\"email\": email[\"from\"].strip()}\n",
    "    if email[\"X-from\"] and email[\"X-from\"].strip() != email[\"from\"]:\n",
    "        sender[\"name\"] = email[\"X-from\"].strip()\n",
    "    email_dict[\"sender\"] = sender\n",
    "\n",
    "    recipients = []\n",
    "    for type_ in [\"to\", \"cc\", \"bcc\"]:\n",
    "        recipient_names = email.get(f\"X-{type_}\", \"\").strip().split(\",\")\n",
    "        recipient_emails = email.get(type_, None)\n",
    "        if recipient_emails is None:\n",
    "            continue\n",
    "        recipient_emails = recipient_emails.split(\",\")\n",
    "        if len(recipient_emails) != len(recipient_names):\n",
    "            recipient_names = [\"\"] * len(recipient_emails)\n",
    "        for recipient_name, recipient_email in zip(recipient_names, recipient_emails):\n",
    "            recipient = {\"type\": type_, \"email\": recipient_email.strip()}\n",
    "            if recipient_name and recipient_name != recipient_email:\n",
    "                recipient[\"name\"] = recipient_name.strip()\n",
    "            recipients.append(recipient)\n",
    "\n",
    "    email_dict[\"recipients\"] = list(sorted(recipients, key=lambda x: x[\"email\"]))\n",
    "\n",
    "    return EmailInformation.model_validate(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "extracted_information = extract_information_with_builtin_parser(sample_raw_email)\n",
    "builtin_test_accuracies = evaluate_extraction(\n",
    "    extract_information_with_builtin_parser, test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this approach on the sample test email, we extract the following information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"```json\n",
    "{extracted_information.model_dump_json(indent=4)}\"\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the average accuracy of this approach on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"{np.mean(builtin_test_accuracies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the test accuracies for each sample as a box plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | fig-cap: Builtin Email Parser Accuracy\n",
    "# | fig-alt: Builtin Email Parser Accuracy\n",
    "\n",
    "ax = sns.boxplot(x=builtin_test_accuracies)\n",
    "ax.set_xlim(left=-0.1, right=1.1)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "sns.despine(offset=10, trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the extraction is quite good, but it's not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach - Use LLM zero-shot extraction with JSON schema\n",
    "\n",
    "We will use [llama-cpp-python](https://llama-cpp-python.readthedocs.io/en/latest/), a python wrapper for [llama.cpp](https://github.com/ggerganov/llama.cpp), to run an LLM locally.\n",
    "\n",
    "It has support for passing a json schema to enforce structured output generation without having to play around with the prompt and retrying in case of failed json generation.\n",
    "\n",
    "> **Note**: There [known performance issues](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md#troubleshooting) with llama.cpp's structured output generation using grammars and, by extension, json schemas especially with nested objects.\n",
    "\n",
    "We use a quantized version of Llama 3.2 3B Instruct as LLM and limit the context length to 16384 in order to handle long raw emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: false\n",
    "# | output: false\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    \"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-3B-Instruct-Q8_0.gguf\",\n",
    "    n_ctx=16384,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a system prompt with instructions for the LLM. Inspired by [this blog post](https://www.boundaryml.com/blog/type-definition-prompting-baml), I decided to use a JSON type definition in the prompt instead of a JSON schema because it is shorter, produced better results and is more human-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n",
    "\n",
    "{\n",
    "    \"date\": string,\n",
    "    \"subject\": string,\n",
    "    \"sender\": {\n",
    "        \"name\": string | null,\n",
    "        \"email\": string,\n",
    "        \"phone_number\": string | null,\n",
    "        \"role\": string | null,\n",
    "        \"organization\": string | null\n",
    "    },\n",
    "    \"recipients\": {\n",
    "        \"name\": string | null,\n",
    "        \"email\": string,\n",
    "        \"phone_number\": string | null,\n",
    "        \"role\": string | null,\n",
    "        \"organization\": string | null,\n",
    "        \"to\": enum([\"to\", \"cc\", \"bcc\"])\n",
    "    }[]\n",
    "}\n",
    "\"\"\"\n",
    "print(f\"\"\"```json\n",
    "{system_prompt}\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: \"**extract_information_with_llm**: Function that extracts information using an LLM\"\n",
    "\n",
    "\n",
    "def extract_information_with_llm(\n",
    "    raw_email: str, *, system_prompt: str\n",
    ") -> EmailInformation:\n",
    "    \"\"\"Extracts structured information from a raw email using an LLM.\n",
    "\n",
    "    Uses chat completion API to parse email content into structured format.\n",
    "    Enforces output schema validation using EmailInformation model specification.\n",
    "\n",
    "    Args:\n",
    "        raw_email: Raw email text including headers and body.\n",
    "        system_prompt: System prompt for the LLM that defines the extraction task.\n",
    "\n",
    "    Returns:\n",
    "        Structured object containing the extracted information, validated against the EmailInformation schema.\n",
    "    \"\"\"\n",
    "    response_format = {\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": EmailInformation.model_json_schema(),\n",
    "    }\n",
    "    output = llm.create_chat_completion_openai_v1(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": raw_email},\n",
    "        ],\n",
    "        response_format=response_format,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    extracted_information = EmailInformation.model_validate_json(\n",
    "        output.choices[0].message.content\n",
    "    )\n",
    "    return extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "extracted_information = extract_information_with_llm(\n",
    "    sample_raw_email, system_prompt=system_prompt\n",
    ")\n",
    "llm_test_accuracies = evaluate_extraction(\n",
    "    partial(extract_information_with_llm, system_prompt=system_prompt), test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"Sample extracted information with llm zero-shot prompt:\n",
    "\n",
    "```json\n",
    "{extracted_information.model_dump_json(indent=4)}\"\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again compute the average accuracy of this next approach on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"{np.mean(llm_test_accuracies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | fig-cap: LLM Zero-Shot Prompt Accuracy\n",
    "# | fig-alt: LLM Zero-Shot Prompt Accuracy\n",
    "\n",
    "ax = sns.boxplot(x=llm_test_accuracies)\n",
    "ax.set_xlim(left=-0.1, right=1.1)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "sns.despine(offset=10, trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this approach performs much worse than the previous one, most likely due to mismatch in expected formats. For example, we expected the date to be of the form `11.07.2001` instead of `11/07/2001`, `11 Jul 2001` or `Wed, 11 Jul 2001`.\n",
    "\n",
    "To fix that, we will provide an example of information extraction in the system prompt in order to better guide the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach - LLM few-shot extraction with JSON schema\n",
    "\n",
    "Due to context length and performance constraints, we will use one example in the system prompt making this a one-shot prompt approach.\n",
    "\n",
    "In order to determine which example to use from the train set, we will add each one to the prompt and evaluate the accuracy improvement on the remaining examples in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_improvements = []\n",
    "\n",
    "for i in trange(len(train_set), desc=\"Example\"):\n",
    "    example = train_set[i]\n",
    "    train_set_without_example = train_set[:i] + train_set[i + 1 :]\n",
    "\n",
    "    # We first compute the accuracy with the zero-shot system prompt\n",
    "    accuracies_zero_shot = evaluate_extraction(\n",
    "        partial(extract_information_with_llm, system_prompt=system_prompt),\n",
    "        train_set_without_example,\n",
    "    )\n",
    "    mean_accuracy_zero_shot = np.mean(accuracies_zero_shot).item()\n",
    "\n",
    "    # We then compute the accurcy with the one-shot (with example) system prompt\n",
    "    system_prompt_with_example = (\n",
    "        system_prompt\n",
    "        + f\"\"\"\n",
    "Use the following example of raw email and extracted information as reference:\n",
    "\n",
    "# Raw email\n",
    "\n",
    "{example[\"raw_email\"]}\n",
    "\n",
    "# Extracted information\n",
    "\n",
    "{example[\"extracted_information\"].model_dump_json(indent=2)}\n",
    "\"\"\"\n",
    "    )\n",
    "    accuracies_one_shot = evaluate_extraction(\n",
    "        partial(extract_information_with_llm, system_prompt=system_prompt_with_example),\n",
    "        train_set_without_example,\n",
    "    )\n",
    "    mean_accuracy_one_shot = np.mean(accuracies_one_shot).item()\n",
    "    # We then compute the difference in accuracy\n",
    "    accuracy_improvement = mean_accuracy_one_shot - mean_accuracy_zero_shot\n",
    "    accuracy_improvements.append(\n",
    "        (\n",
    "            accuracy_improvement,\n",
    "            system_prompt_with_example,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | fig-cap: LLM One-Shot Prompt Accuracy Improvement\n",
    "# | fig-alt: LLM One-Shot Prompt Accuracy Improvement\n",
    "\n",
    "ax = sns.boxplot(x=[x[0] for x in accuracy_improvements])\n",
    "ax.set_xlabel(\"Accuracy Improvement\")\n",
    "sns.despine(offset=10, trim=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "best_index = np.argmax([x[0] for x in accuracy_improvements])\n",
    "best_accuracy_improvement = accuracy_improvements[best_index][0]\n",
    "best_system_prompt_with_example = accuracy_improvements[best_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"The best accuracy improvement we can obtain using the one-shot prompt and out small train set is: {best_accuracy_improvement * 100:.2f}\n",
    "\n",
    "The system prompt and, by extension, example corresponding to this improvement are:\n",
    "\n",
    "```json\n",
    "{best_system_prompt_with_example}\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "extracted_information = extract_information_with_llm(\n",
    "    sample_raw_email, system_prompt=best_system_prompt_with_example\n",
    ")\n",
    "few_shot_llm_test_accuracies = evaluate_extraction(\n",
    "    partial(\n",
    "        extract_information_with_llm, system_prompt=best_system_prompt_with_example\n",
    "    ),\n",
    "    test_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"\"\"Sample extracted information with llm one-shot prompt:\n",
    "\n",
    "```json\n",
    "{extracted_information.model_dump_json(indent=4)}\"\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy of this second approach on the test set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: asis\n",
    "\n",
    "print(f\"{np.mean(few_shot_llm_test_accuracies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | fig-cap: LLM One-Shot Prompt Accuracy\n",
    "# | fig-alt: LLM One-Shot Prompt Accuracy\n",
    "\n",
    "ax = sns.boxplot(x=few_shot_llm_test_accuracies)\n",
    "ax.set_xlim(left=-0.1, right=1.1)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "sns.despine(offset=10, trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the 3 approaches\n",
    "\n",
    "If we plot the accuracies of each approach side-by-side, we can clearly see that both the approach with the built-in parser and the one-shot prompt approach outperform the approach with the zero-shot prompt.\n",
    "\n",
    "The one-shot prompt is more flexible than the one with the built-in parser and would generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | fig-cap: LLM One-Shot Prompt Accuracy\n",
    "# | fig-alt: LLM One-Shot Prompt Accuracy\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Builtin\": builtin_test_accuracies,\n",
    "        \"LLM Zero-Shot\": llm_test_accuracies,\n",
    "        \"LLM One-Shot\": few_shot_llm_test_accuracies,\n",
    "    }\n",
    ")\n",
    "\n",
    "ax = sns.boxplot(data=df)\n",
    "ax.set_ylim(bottom=-0.1, top=1.1)\n",
    "ax.set_xlabel(\"Approach\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "sns.despine(offset=10, trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this post, we have seen how easy it is to use a local LLM to extract structured information from emails (as long as you access to the emails' content and are allowed to do so). We have also seen how to add an example to the prompt (one-shot prompt) to improve the accuracy of the extraction.\n",
    "\n",
    "At last, I would to conclude with stating that is just a showcase of how one could extract structured information and not a real tutorial. For any real world use case, you would probably have more constraints and would require using a better metric, a better LLM and more ground truth data to evaluate and improve the approach.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_information_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
