{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: 2024-11-25\n",
    "title: \"Using an LLM for information extraction\"\n",
    "keywords: [\n",
    "    \"Enron email dataset\",\n",
    "    \"Information extraction\",\n",
    "    \"LLM\",\n",
    "    \"NLP\",\n",
    "    \"llama.cpp\"\n",
    "]\n",
    "license: \"CC BY\"\n",
    "execute: \n",
    "  cache: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will see how to use a local LLM to extract structured information from emails.\n",
    "\n",
    "My very first project when I started working at appliedAI Initiative in 2021, involved information extraction from emails for a company that makes a document management system. Back then LLMs were not yet as widespread and as useful as they are right now, so we decided to train a model from scratch. We however didn't have any labelled data for training because we couldn't use their customer data due to privacy reasons and had to resort to manually labelling emails from the [Enron email dataset]() and in the end the results were not very impressive.\n",
    "\n",
    "Now, this type of application is simpler than ever and I want to demonstrate that in this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import yaml\n",
    "from email.parser import Parser\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Literal\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff\n",
    "from llama_cpp import Llama\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(BaseModel):\n",
    "    name: str | None = None\n",
    "    email: str\n",
    "    phone_number: str | None = None\n",
    "    role: str | None = None\n",
    "    organization: str | None = None\n",
    "\n",
    "\n",
    "class Recipient(Sender):\n",
    "    type: Literal[\"to\", \"cc\", \"bcc\"] = \"to\"\n",
    "\n",
    "\n",
    "class EmailInformation(BaseModel):\n",
    "    date: str\n",
    "    subject: str\n",
    "    sender: Sender\n",
    "    recipients: list[Recipient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_extracted_information_accuracy(\n",
    "    *,\n",
    "    expected_info: EmailInformation,\n",
    "    extracted_info: EmailInformation,\n",
    ") -> float:\n",
    "    extracted_info_dict = extracted_info.model_dump(mode=\"json\", exclude_none=True)\n",
    "    expected_info_dict = expected_info.model_dump(mode=\"json\", exclude_none=True)\n",
    "    diff_result = DeepDiff(\n",
    "        expected_info_dict,\n",
    "        extracted_info_dict,\n",
    "        get_deep_distance=True,\n",
    "        verbose_level=2,\n",
    "    )\n",
    "    return 1 - diff_result[\"deep_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction(\n",
    "    extract_fn: Callable[[str], EmailInformation],\n",
    "    dataset: list[dict[str, Any]],\n",
    ") -> list[float]:\n",
    "    accuracies = []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"Emails\"):\n",
    "        print(f\"{sample=}\")\n",
    "        extracted_information = extract_fn(sample[\"raw_email\"])\n",
    "\n",
    "        accuracy = compute_extracted_information_accuracy(\n",
    "            extracted_info=extracted_information,\n",
    "            expected_info=sample[\"extracted_information\"],\n",
    "        )\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Similarly to my first project, we will use as data emails from the [Enron dataset](https://www.cs.cmu.edu/~enron/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"enron_emails.yml\") as f:\n",
    "    all_email_data = yaml.safe_load(f)\n",
    "\n",
    "for email_data in all_email_data:\n",
    "    email_data[\"extracted_information\"] = EmailInformation.model_validate(\n",
    "        email_data[\"extracted_information\"]\n",
    "    )\n",
    "\n",
    "all_indices = list(range(len(all_email_data)))\n",
    "train_set_indices = random.choices(all_indices, k=int(0.5 * len(all_email_data)))\n",
    "test_set_indices = list(set(all_indices).difference(train_set_indices))\n",
    "train_set = [all_email_data[i] for i in train_set_indices]\n",
    "test_set = [all_email_data[i] for i in test_set_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_raw_email, sample_email_information = train_set[0]\n",
    "print(f\"Sample raw email:\\n{train_set[0]['raw_email']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][\"extracted_information\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach - Use Python's builtin email parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_with_builtin_parser(raw_email: str) -> EmailInformation:\n",
    "    parser = Parser()\n",
    "    email = parser.parsestr(raw_email)\n",
    "    email_dict = {\"date\": email[\"date\"].strip(), \"subject\": email[\"subject\"].strip()}\n",
    "    sender = {\"email\": email[\"from\"].strip()}\n",
    "    if email[\"X-from\"] and email[\"X-from\"].strip() != email[\"from\"]:\n",
    "        sender[\"name\"] = email[\"X-from\"].strip()\n",
    "    email_dict[\"sender\"] = sender\n",
    "\n",
    "    recipients = []\n",
    "    for type_ in [\"to\", \"cc\", \"bcc\"]:\n",
    "        recipient_names = email.get(f\"X-{type_}\", \"\").strip().split(\",\")\n",
    "        recipient_emails = email.get(type_, None)\n",
    "        if recipient_emails is None:\n",
    "            continue\n",
    "        recipient_emails = recipient_emails.split(\",\")\n",
    "        if len(recipient_emails) != len(recipient_names):\n",
    "            recipient_names = [\"\"] * len(recipient_emails)\n",
    "        for recipient_name, recipient_email in zip(recipient_names, recipient_emails):\n",
    "            recipient = {\"type\": type_, \"email\": recipient_email.strip()}\n",
    "            if recipient_name and recipient_name != recipient_email:\n",
    "                recipient[\"name\"] = recipient_name.strip()\n",
    "            recipients.append(recipient)\n",
    "\n",
    "    email_dict[\"recipients\"] = list(sorted(recipients, key=lambda x: x[\"email\"]))\n",
    "\n",
    "    return EmailInformation.model_validate(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information = extract_information_with_builtin_parser(\n",
    "    test_set[0][\"raw_email\"]\n",
    ")\n",
    "extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accuracy = compute_extracted_information_accuracy(\n",
    "    extracted_info=extracted_information,\n",
    "    expected_info=test_set[0][\"extracted_information\"],\n",
    ")\n",
    "print(f\"Sample email information extraction accuracy: {sample_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = evaluate_extraction(extract_information_with_builtin_parser, test_set)\n",
    "\n",
    "mean_test_accuracy = np.mean(test_accuracies).item()\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy for builtin parser: {mean_test_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach - Use LLM with Json schema\n",
    "\n",
    "We will use [llama-cpp-python](), a python wrapper for [llama.cpp](), to run an LLM locally.\n",
    "\n",
    "It has support for passing a json schema to enforce structured output generation without having to play around with the prompt and retrying in case of failed json generation.\n",
    "\n",
    "Unfortunately, due to [known performance issues](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md#troubleshooting) with llama.cpp's grammars and, by extension, json schemas, we will instead use [instructor]() ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    \"bartowski/Llama-3.2-1B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-1B-Instruct-Q8_0.gguf\",\n",
    "    n_ctx=16384,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    ")\n",
    "llm_extract_information = instructor.patch(\n",
    "    create=llm.create_chat_completion_openai_v1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_json_schema = {\n",
    "    \"type\": \"json_object\",\n",
    "    \"schema\": EmailInformation.model_json_schema(),\n",
    "}\n",
    "print(email_json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n",
    "\n",
    "{json.dumps(email_json_schema, indent=4)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_with_llm(\n",
    "    raw_email: str, *, system_prompt: str\n",
    ") -> EmailInformation:\n",
    "    extracted_information = llm_extract_information(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": raw_email},\n",
    "        ],\n",
    "        response_model=EmailInformation,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information = extract_information_with_llm(\n",
    "    test_set[0][\"raw_email\"], system_prompt=system_prompt\n",
    ")\n",
    "extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accuracy = compute_extracted_information_accuracy(\n",
    "    extracted_info=extracted_information,\n",
    "    expected_info=test_set[0][\"extracted_information\"],\n",
    ")\n",
    "print(f\"Sample email information extraction accuracy: {sample_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = evaluate_extraction(\n",
    "    partial(extract_information_with_llm, system_prompt=system_prompt), test_set\n",
    ")\n",
    "\n",
    "mean_test_accuracy = np.mean(test_accuracies).item()\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy: {mean_test_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third approach - LLM with Json schema and few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = evaluate_extraction(\n",
    "    partial(extract_information_with_llm, system_prompt=system_prompt), train_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "\n",
    "for i in range(train_set):\n",
    "    example = train_set[i]\n",
    "    train_set_without_example = train_set[:i] + train_set[i + 1 :]\n",
    "    system_prompt_with_example = f\"\"\"You are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n",
    "\n",
    "{json.dumps(email_json_schema, indent=4)}\n",
    "\n",
    "Use the following example as reference:\n",
    "{example[\"extracted_information\"]}\n",
    "\"\"\"\n",
    "    accuracies = evaluate_extraction(\n",
    "        partial(extract_information_with_llm, system_prompt=system_prompt_with_example),\n",
    "        train_set_without_example,\n",
    "    )\n",
    "    mean_accuracy = np.mean(accuracies).item()\n",
    "    train_accuracies.append((mean_accuracy, system_prompt_with_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmin([x[0] for x in train_accuracies])\n",
    "best_system_prompt_with_example = train_accuracies[best_index][1]\n",
    "print(best_system_prompt_with_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = evaluate_extraction(\n",
    "    partial(\n",
    "        extract_information_with_llm, system_prompt=best_system_prompt_with_example\n",
    "    ),\n",
    "    test_set,\n",
    ")\n",
    "\n",
    "mean_test_accuracy = np.mean(test_accuracies).item()\n",
    "print(\n",
    "    f\"Mean email information extraction test accuracy: {mean_test_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this post, we have ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_information_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
