[
  {
    "objectID": "posts/control-series/01-introduction/index.html",
    "href": "posts/control-series/01-introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "In this first post of the Control series, we will introduce the system that we want to control as well as the different control objectives. I am writing this series because I miss working on Control problems.\nThe aim of this series is to apply as many methods to model, control and observe the chosen system as possible in order for me to get back some of the knowledge I lost while away from control engineering as well as gain some new knowledge."
  },
  {
    "objectID": "posts/control-series/01-introduction/index.html#modeling",
    "href": "posts/control-series/01-introduction/index.html#modeling",
    "title": "Introduction",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Series\n\nControl Series\nThis series contains blog plots about different ways to control a specific system.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSeries: Control\n\n\n\n\n\n\n\n\n\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\nUsing an LLM for information extraction\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\nPoetry and Python Versions\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\nWhen ECR lifecycle policies are not enough…\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2020\n\n\nAnes Benmerzoug\n\n\n\n\n\n\n\n\n\n\n\n\nModel Versioning with MLflow\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\nAnes Benmerzoug\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/poetry-and-python-versions/index.html",
    "href": "posts/poetry-and-python-versions/index.html",
    "title": "Poetry and Python Versions",
    "section": "",
    "text": "After a long pause of more than 4 years, I have finally decided to come back to my personal blog and start writing again.\nFor this first post in a long while, I wanted to do something simple: benchmark the performance of Poetry the python package manager when using different Python versions. The idea came to me after I saw that Python 3.13 was released with experimental free threading support where the global interpreter lock (GIL) is disabled.\nAs a daily user of Poetry either for work or for personal projects, I was curious to see whether its performance changes significantly based on the Python version used. To answer this question, I decided to take inspiration from the python-package-manager-shootout repository, which benchmarks different Python package managers using a fixed Python version.\n\nUnfortunately, as of writing this blog post Poetry can’t installed be directly from PyPI using Pyhton 3.13 with the GIL disabled due to an issue with some of its dependencies, namely msgpack and cryptography. once that’s resolved, I will try to update this post or write another one.\n\nThe remainder of this post is structured as follows:\n\nwe will briefly see how to install the new Python version with free threading enabled,\nthen, we will describe the different operations as well as the setup that will used in the benchmarks\nfinally, we will visually analyze the result of the benchmarks.\n\n\nDisclaimer This experiment has several limitations that should be considered when interpreting the results. Firstly, we’re only testing with a limited set of Python versions (namely 3.11, 3.12, and 3.13). Additionaly, we’re using a fixed set of packages (Sentry’s requirements.txt file) to test Poetry’s performance. Both of these choices may not capture the full range of use cases and scenarios that Poetry may encounter in real-world use cases.\n\n\nInstallation\nWe will now see how to install Python 3.13 version with and without free threading using pyenv, a popular tool for managing multiple Python versions.\nLet’s first start by listing all available python versions for installation:\npyenv install -l\nThe result should look something like:\nAvailable versions:\n  2.1.3\n  2.2.3\n  2.3.7\n  ...\n  ...\n  3.13.0\n  3.13.0t\n  ...\n  ...\nIf you don’t see python 3.13 listed, it probably means that you need to update pyenv to the latest version using:\npyenv update\nTo install Python 3.13 with free threading disabled (i.e. GIL enabled), you would use:\npyenv install 3.13.0\nTo install Python 3.13 with free threading enabled (e.g. GIL disabled), you would use:\npyenv install 3.13.0t\nTo test whether this worked, you would use:\npython -VV\nThe output should contain “experimental free-threading build” for the latter and not for the former.\nFor more detailed instructions and explanations, please refer to this blog post from realpython.com\n\n\nMethodology\nTo benchmark Poetry’s performance, I used the latest version of Poetry (version 1.8.4 as of writing this blog post) with a few different Python versions: 3.11, 3.12 and 3.13\nAll the files related to this blog post can be found in this repository. It is heavily inspired by the python-package-manager-shootout repository.\nSimilarly to that repository, we use a list of packages from a fixed version of Sentry’s requirements.txt file which was chosen arbitrarily as a non-trivial real-world example.\nUnlike in python-package-manager-shootout, we use a newer version of Sentry’s requirements to avoid any issues during package installation with newer Python versions. Additionally, we use hyperfine, a command-line benchmarking tool, to handle the execution and timing of each operation.\nI benchmarked the following operations:\n\nimport: For this operation, we call poetry add --lock --no-cache to import all of the requirements in Sentry’s requirements.txt file into a newly initialized pyproject.toml file (We use Poetry’s --lock flag to prevent it from installing packages at this point).\nlock: For this operation, we call poetry lock both with and without the --no-cache. We delete the poetry.lock before every call to make sure the lock creation starts from scratch each time.\ninstall: For this operation, we call poetry install both with and without the --no-cache. We delete the created virtual environment before every call to make sure the installation starts from scratch each time.\nupdate: For this operation, we call poetry update both with and without the --no-cache. We delete the poetry.lock and restore it before every call to make sure the update starts from the same point each time.\nadd package: For this operation, we call poetry add numpy --no-cache. We remove the numpy package after every call to make sure adding the package starts from the same point each time.\n\n\nThe --no-cache flag is used to tell poetry to ignore its own cache stored, by default on Linux, under ~/.cache/pypoetry/cache.\n\n\n\nResults\nI ran the benchmarks in CI to have a more or less consistent environment. The workflow runs the operations, collects and combines the results and then creates plots based on them.\nThe results and plots can be found as artifacts of the repository’s benchmark CI workflow. The plots used in this post come from this specific workflow, from the benchmark-plots artifact.\nIn Figure 1, we can see that poetry with python 3.13 performs the best on average for the import operation.\n\n\n\n\n\n\nFigure 1: Benchmark result of poetry import operation\n\n\n\nIn Figure 2, we can see that poetry with python 3.13 performs the best on average for the lock operation both with and without caching.\n\n\n\n\n\n\nFigure 2: Benchmark result of poetry lock operation\n\n\n\nIn Figure 3, we can see that poetry with python 3.11 performs the best on average for the install operation both with and without caching.\n\n\n\n\n\n\nFigure 3: Benchmark result of poetry install operation\n\n\n\nIn Figure 4, we can see that poetry with python 3.13 performs the best on average for the update operation both with and without caching.\n\n\n\n\n\n\nFigure 4: Benchmark result of poetry update operation\n\n\n\nIn Figure 5, we can see that poetry with python 3.11 and 3.13 perform similarily on average for the add package operation.\n\n\n\n\n\n\nFigure 5: Benchmark result of poetry add package operation\n\n\n\nAs we can see from the results, python 3.13 without free threading does improve poetry’s performance on average in 3 out of the 5 operations (import, lock, update) both with and without caching. In the other 2 operations, the performance difference is minimal.\n\n\nConclusion\nIn conclusion, our little experiment has shown that Poetry with Python 3.13 performs better than Poetry with Python 3.11 and 3.12 on average in most cases. However the performance is not that significant, and it remains to be seen whether the experimental free threading feature would make a significant difference.\nFor now, I will keep on eye on Python 3.13 with free threading’s support for Poetry, and update my findings as more information becomes available. This experiment is just one of many possible tests of Poetry’s performance with different Python versions, and the results should be taken as a rough indication only as they may not necessarily reflect real-world use cases.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a Senior AI Engineer at appliedAI Initiative, Europe’s largest initiative for the application of leading edge trustworthy AI technology with the vision to shape Europe’s innovative power in AI.\nI’m passionate about machine learning, software engineering, and open-source development.\nI am currently working on LLM and NLP, more specifically RAG (Retrieval-Augmented Generation), projects for different clients. When I’m not staffed on client projects, I work on internal projects, contribute to open-source projects such as pyDVL and help with TransferLab’s research projects such as the replication of the If You Like Shapley Then You’ll Love the Core paper.\nI hold a M.Sc. in Embedded Systems from the Technical University of Kaiserslautern and a M.Sc. in Control and Automation from the Ecole Nationale Polytechnique in Algiers."
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nappliedAI Initiative GmbH\n\nSenior Artificial Intelligence Engineer\nJuly 2023 - Present\nMunich, Germany\n\nappliedAI - UnternehmerTUM\n\nArtificial Intelligence Engineer\nJune 2021 - July 2023\nMunich, Germany\n\nAvira\n\nSpecialist Artificial Intelligence Researcher\nOctober 2018 - May 2021\nTettnang, Germany\n\nFraunhofer ITWM\n\nResearch Assistant (HiWi)\nJuly 2018 - September 2018\nKaiserslautern, Germany"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nM.Sc. in Embedded Systems\n\nTechnical University of Kaiserslautern\nSeptember 2016 - October 2018\n\nM.Sc. in Automation and Control Systems\n\nEcole Nationale Polytechnique Algiers\nSeptember 2013 - July 2016"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects\n\npyDVL An open-source library of stable implementations of algorithms for data valuation and influence function computation.\nLangsfer An open-source library for language transfer methods and algorithms."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\nLanguages\n\nPython\nRust\nSQL\n\n\n\nAI\n\nMachine Learning\nDeep Learning\nLarge Language Models (LLMs)\nRetrieval Augmented Generation (RAG)\nNatural Language Processing (NLP)\nData Valuation\nExplainability (XAI)\n\n\n\nTechnologies and Services\n\nContainerization: Docker\nOrchestration: Kubernetes\nExperiments: MLflow\nDatabase Systems: PostgreSQL, ElasticSearch\nBackend: FastAPI, Django\nCloud: AWS, Azure"
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html",
    "href": "posts/model-versioning-with-mlflow/index.html",
    "title": "Model Versioning with MLflow",
    "section": "",
    "text": "In this very first post we will talk about machine learning model versioning and more specifically machine learning classifier versioning.\nOf course, one could simply compare accuracies ( or whichever metric you’re using ) on a separate test set and promote whichever classifier has a better value but that does not offer us the same guarantees as statistical tests.\nFor example, one may train two different classifiers on the same dataset and get the following results on a separate test set:\nWe can see that the first classifier has an accuracy of 60% and the second classifier an accuracy of 50%. If we were to stop here we would just say that the first classifier is better than the second one and that could be true, but can this result be trusted?\nTo verify that we construct the following confusion matrix:\nBy comparing the off-diagonal elements, we can intuitively know that there isn’t much of a difference between the two classifiers. To make that more precise we can use statistical tests instead of just comparing numbers.\nOne such test is McNemar’s Test.\nBefore explaining McNemar’s Test and the different steps used to compare two machine learning classifiers, let’s first talk about MLflow."
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html#mlflow",
    "href": "posts/model-versioning-with-mlflow/index.html#mlflow",
    "title": "Model Versioning with MLflow",
    "section": "MLFlow",
    "text": "MLFlow\nMLflow is an open source platform for the machine learning life-cycle. It is currently composed of four components:\n\nMLflow Tracking: Used to record and query experiments: code, data, config, and results\nMLflow Projects: Used to package data science code in a format to reproduce runs on any platform\nMLflow Models: Used to deploy machine learning models in diverse serving environments\nMLflow Model Registry: Used to store, annotate, discover, and manage models in a central repository\n\nIt uses a classic client server architecture as depicted in the following diagram:\n\n\n\n\n\nflowchart TD\n    subgraph MLflow\n        database[(Database)] &lt;--&gt; server(Server)\n        storage[(Storage)] &lt;--&gt; server\n    end\n\n    server &lt;--&gt; client(Client)\n\n\n\n\n\n\nThe Client, user, interacts directly with the Server and the Server in turn interacts with the Database (MySQL, MSSQL, SQLITE, or POSTGRESQL) and the Storage backend (Local or Cloud).\nIn this post we’re only interested in the last component: the Model Registry.\nIt is a centralized model store, set of APIs, and UI, to collaboratively manage the full life-cycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, stage transitions (such as from staging to production), and annotations.\nA registered model can be in any one of the following stages: - None - Staging - Production - Archived\nAs can be seen in the following flowchart, a model starts, when first logged or registered, in the None stage and then transitions to the Staging stage, then to the Production stage and finally end its life-cycle in the Archived stage.\n\n\n\n\n\nflowchart LR\n    none(None)\n    staging(Staging)\n    production(Production)\n    archived(Archived)\n\n    none --&gt; staging\n    staging --&gt; production\n    production --&gt; archived\n\n\n\n\n\n\nFor simplicity’s sake we won’t consider other possible transitions (e.g. Staging -&gt; Archived).\nWhat the Model Registry does not take care of is automatically transition a given model to the appropriate stage and that is understandable because the conditions needed to do that depend on the actual application."
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html#mcnemars-test",
    "href": "posts/model-versioning-with-mlflow/index.html#mcnemars-test",
    "title": "Model Versioning with MLflow",
    "section": "McNemar’s test",
    "text": "McNemar’s test\nMcNemar’s test is a non-parametric statistical test that can be used to compare two classification models by constructing a 2x2 contingency table, or confusion matrix, like the following:\n\n\n\n\nModel 2 Correct\nModel 2 Wrong\n\n\n\n\nModel 1 Correct\na\nb\n\n\nModel 1 Wrong\nc\nd\n\n\n\nIn order to test if there is a significant difference between the two models, we use only the off-diagonal elements, b and c, since the other elements tell us nothing about whether one model is better than the other or not.\nMcNemar’s test statistic is:\n\\[\nQ = \\frac{(b - c)}{b + c}\n\\]\nWhich, for large values of b and c, follows a chi-squared distribution with 1 degree of freedom \\(\\chi_{1}^{2}\\).\nTo more closely approximate the chi-squared distribution we can use the following definition instead which contains a continuity correction:\n\\[\nQ = \\frac{(|b - c| - 1)}{b + c}\n\\]\nIf the result is significant, i.e. greater than a pre-defined significance level, usually set to 0.05 but can be changed depending on the use case, then we can conclude that the two models are significantly different from each other.\nBut it does not end there, we still have to determine which one of the two is better than the other one. For that, we can use one or a combination of the usual metrics: Accuracy, F-Score, False Positive Rate, etc.\nIf we apply the continuity corrected version of the test on our previous example we get as result 1.0 and can confidently say that there is no significant difference between the two classifiers."
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html#model-versioning-flow",
    "href": "posts/model-versioning-with-mlflow/index.html#model-versioning-flow",
    "title": "Model Versioning with MLflow",
    "section": "Model Versioning Flow",
    "text": "Model Versioning Flow\nNow that we have defined and explained all the required parts of the flow. We can assemble them into the following chart that shows the different steps taken to compare two different classification models:\n\n\n\n\n\ngraph TD\n    End--&gt;title[\"Model Versioning Flow Chart\"]\n    style title fill:#FFF,stroke:#FFF\n    linkStyle 0 stroke:#FFF,stroke-width:0;\n    Start((Start)) --&gt; A\n    A(McNemar's Test) --&gt; B{\"P-value &lt; &alpha;\"}\n    B --&gt;|No| End((End))\n    B --&gt;|Yes| C(Compute&lt;br&gt;Accuracies)\n    C --&gt; D{Model1 Accuracy&lt;br&gt;&lt;&lt;br&gt;Model2 Accuracy}\n    D --&gt;|No| End\n    D --&gt;|Yes| E(Deploy Model2)\n    E --&gt; End\n\n\n\n\n\n\nHere we use accuracy, but it could be replaced by other metrics such as False Positive Rate, False Negative Rate, etc."
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html#example",
    "href": "posts/model-versioning-with-mlflow/index.html#example",
    "title": "Model Versioning with MLflow",
    "section": "Example",
    "text": "Example\nIn this repository you can find example code in Python that shows how to use the previous flow to do model versioning for machine learning classifiers with MLflow.\nOne important thing that should always be done is to pin the random seed to ensure the experiment’s repeatability.\nrandom_seed = 16\nnp.random.seed(random_seed)\nIn the example, we start off by generating artificial classification data using scikit-learn’s make_classification helper function and then splitting it into a training and a testing set:\nX, y = make_classification(\n        n_samples=10000,\n        n_classes=2,\n        n_features=20,\n        n_informative=9,\n        random_state=random_seed,\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=0.8, test_size=0.2\n)\nAfter that, we fit a Logistic Regression classifier, then register and log it into MLflow and finally move it to the Production phase:\nwith mlflow.start_run():\n    lr_model = LogisticRegression()\n    lr_model.fit(X_train, y_train)\n    y_pred = lr_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.sklearn.log_model(\n        lr_model, artifact_path=\"model\", registered_model_name=\"Logistic Regression\"\n    )\n\nmlflow_client.transition_model_version_stage(\n    name=\"Logistic Regression\", version=1, stage=\"Production\"\n)\nThen, we fit a Random Forest classifier, then register and log it into MLflow and finally move it to the Staging phase:\nwith mlflow.start_run():\n    rf_model = RandomForestClassifier()\n    rf_model.fit(X_train, y_train)\n    y_pred = rf_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.sklearn.log_model(\n        rf_model, artifact_path=\"model\", registered_model_name=\"Random Forest\"\n    )\n\nmlflow_client.transition_model_version_stage(\n    name=\"Random Forest\", version=1, stage=\"Staging\"\n)\nTo simulate the fact that the model comparison may happen in another script we delete both trained model instances and load them back from MLflow:\ndel lr_model\ndel rf_model\n\nlr_model_download_uri = mlflow_client.get_model_version_download_uri(\n    name=\"Logistic Regression\", version=1,\n)\nrf_model_download_uri = mlflow_client.get_model_version_download_uri(\n    name=\"Random Forest\", version=1,\n)\nlr_model = mlflow.sklearn.load_model(lr_model_download_uri)\nrf_model = mlflow.sklearn.load_model(rf_model_download_uri)\nAs a next step, we use both models to generate predictions on the test set. We use these predictions to compute each model’s accuracy and to create a contingency table that is finally used in a corrected version of McNemar’s Test to return a P-value:\ny_pred_lr = lr_model.predict(X_test)\ny_pred_rf = rf_model.predict(X_test)\n\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\ncontingency_table = mcnemar_table(y_test, y_pred_lr, y_pred_rf)\n_, p_value = mcnemar(contingency_table, corrected=True)\nFinally we use the obtained P-value and the accuracies to decide whether we should deploy the Random Forest classifier to Production and archive the Logistic Regression classifier or not:\nif p_value &lt; significance and accuracy_lr &lt; accuracy_rf:\n    mlflow_client.transition_model_version_stage(\n        name=\"Logistic Regression\", version=1, stage=\"Archived\",\n    )\n    mlflow_client.transition_model_version_stage(\n        name=\"Random Forest\", version=1, stage=\"Production\",\n    )\nWe can then access the MLflow server’s dashboard and see that the Random Forest classifier’s version 1 is in Production and the Logistic Regression classifier’s version 1 was archived:\n\n\n\n\nMLflow Model Versioning Screenshot\n\n\n\nAll that’s left now is to run this or similar code either on a schedule or as part of a training workflow each time a new classifier is trained and logged."
  },
  {
    "objectID": "posts/model-versioning-with-mlflow/index.html#conclusion",
    "href": "posts/model-versioning-with-mlflow/index.html#conclusion",
    "title": "Model Versioning with MLflow",
    "section": "Conclusion",
    "text": "Conclusion\nWe have seen that thanks to the Model Registry component of MLflow we can have a pretty simple automated model versioning flow for classifiers. This flow can be and should be extended and made more complete, depending on the use case. For example, by using a second metric for when a tie happens in the first one.\nI hope that you have learned at a thing or two from this post. If there are any mistakes or if you have questions please do not hesitate to reach out to me."
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html",
    "href": "posts/llm-email-information-extaction/index.html",
    "title": "Using an LLM for information extraction",
    "section": "",
    "text": "In this post we will see how to use a local LLM to extract structured information from emails.\nWhen I joined appliedAI Initiative in 2021, my first project involved using AI to extract information from emails for a company that develops a document management system. At that time, LLMs were not as advanced as they are today, so we decided to train a custom model from scratch. However, we faced challenges, particularly because we did not have labeled data, and due to privacy restrictions, we couldn’t use the company’s customer data. As a result, we resorted to manually labeling emails from the Enron dataset1.\nUnfortunately, our approach yielded suboptimal results for several reasons, such as the mismatch between the Enron dataset and the actual customer emails and the lack of labeled training data. Today, however, extracting information from emails has become simpler than ever, and in this post, I want to demonstrate the improvements we can achieve with using LLMs."
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html#baseline---use-pythons-builtin-email-parser",
    "href": "posts/llm-email-information-extaction/index.html#baseline---use-pythons-builtin-email-parser",
    "title": "Using an LLM for information extraction",
    "section": "Baseline - Use Python’s builtin email parser",
    "text": "Baseline - Use Python’s builtin email parser\nAs a baseline, we use Python’s built-in email parser from the email package to extract information. We define an extraction function that parses the emails and extracts basic information without much validation.\n\n\nextract_information_with_builtin_parser: Function that extracts information using Python’s built-in email parser.\ndef extract_information_with_builtin_parser(raw_email: str) -&gt; EmailInformation:\n    \"\"\"Extracts structured information from a raw email using Python's built-in email parser.\n\n    Parses the raw email text to extract metadata including date, subject, sender, and recipients.\n    Handles special X-headers for additional information like sender and recipient names.\n\n    Args:\n        raw_email: Raw email text including headers and body.\n\n    Returns:\n        Structured object containing the extracted information with:\n            - date: Formatted as DD.MM.YYYY\n            - subject: Email subject line\n            - sender: Sender information including email and optional name\n            - recipients: List of recipients (to/cc/bcc) with email and optional name,\n                sorted by email address\n    \"\"\"\n    parser = Parser()\n    email = parser.parsestr(raw_email)\n    parsed_date = datetime.strptime(\n        email[\"date\"].strip().split(\"(\")[0], \"%a, %d %b %Y %H:%M:%S %z \"\n    )\n    formatted_date = parsed_date.strftime(\"%d.%m.%Y\")\n    email_dict = {\"date\": formatted_date, \"subject\": email[\"subject\"].strip()}\n    sender = {\"email\": email[\"from\"].strip()}\n    if email[\"X-from\"] and email[\"X-from\"].strip() != email[\"from\"]:\n        sender[\"name\"] = email[\"X-from\"].strip()\n    email_dict[\"sender\"] = sender\n\n    recipients = []\n    for type_ in [\"to\", \"cc\", \"bcc\"]:\n        recipient_names = email.get(f\"X-{type_}\", \"\").strip().split(\",\")\n        recipient_emails = email.get(type_, None)\n        if recipient_emails is None:\n            continue\n        recipient_emails = recipient_emails.split(\",\")\n        if len(recipient_emails) != len(recipient_names):\n            recipient_names = [\"\"] * len(recipient_emails)\n        for recipient_name, recipient_email in zip(recipient_names, recipient_emails):\n            recipient = {\"type\": type_, \"email\": recipient_email.strip()}\n            if recipient_name and recipient_name != recipient_email:\n                recipient[\"name\"] = recipient_name.strip()\n            recipients.append(recipient)\n\n    email_dict[\"recipients\"] = list(sorted(recipients, key=lambda x: x[\"email\"]))\n\n    return EmailInformation.model_validate(email_dict)\n\n\n\n\n\n\n\nUsing this method, we extract the following information from the sample test email:\n{\n    \"date\": \"10.05.2001\",\n    \"subject\": \"FP&L\",\n    \"sender\": {\n        \"name\": \"Kam Keiser\",\n        \"email\": \"kam.keiser@enron.com\",\n        \"phone_number\": null,\n        \"role\": null,\n        \"organization\": null\n    },\n    \"recipients\": [\n        {\n            \"name\": \"Sabra L Dinari\",\n            \"email\": \"sabra.dinari@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": null,\n            \"type\": \"to\"\n        },\n        {\n            \"name\": \"Scott Neal\",\n            \"email\": \"scott.neal@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": null,\n            \"type\": \"cc\"\n        },\n        {\n            \"name\": null,\n            \"email\": \"scott.neal@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": null,\n            \"type\": \"bcc\"\n        },\n        {\n            \"name\": \"Yuan Tian\",\n            \"email\": \"yuan.tian@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": null,\n            \"type\": \"cc\"\n        },\n        {\n            \"name\": null,\n            \"email\": \"yuan.tian@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": null,\n            \"type\": \"bcc\"\n        }\n    ]\n}\"\nWe also compute the average score of this approach on the test set:\n84.29%\nThe results are plotted as a box plot, showing that while the extraction is relatively good, it is far from perfect.\n\n\n\n\n\nBuiltin Email Parser Score"
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html#first-approach---use-llm-zero-shot-extraction-with-json-schema",
    "href": "posts/llm-email-information-extaction/index.html#first-approach---use-llm-zero-shot-extraction-with-json-schema",
    "title": "Using an LLM for information extraction",
    "section": "First Approach - Use LLM zero-shot extraction with JSON schema",
    "text": "First Approach - Use LLM zero-shot extraction with JSON schema\nNext, we explore using an LLM for zero-shot extraction with a JSON schema. For this, we use llama-cpp-python, a Python wrapper for llama.cpp, to run a local LLM. This approach allows us to pass a JSON schema, ensuring structured output without needing to adjust the prompt repeatedly in order to coerce the LLM into generating valid JSON data.\n\nNote: There are known performance issues with llama.cpp’s structured output generation using grammars and, by extension, json schemas especially with nested objects.\n\nWe use a quantized version of Llama 3.2 3B Instruct with a context length of 16,384 tokens to handle long raw emails.\n\nllm = Llama.from_pretrained(\n    \"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n    filename=\"Llama-3.2-3B-Instruct-Q8_0.gguf\",\n    n_ctx=16384,\n    n_gpu_layers=-1,\n    verbose=False,\n)\n\nA system prompt is defined to guide the LLM, using a JSON type definition instead of a JSON schema, inspired by a blog post I came across. This version is shorter, produces better results, and is more human-readable.\nWe then define a system prompt to guide the LLM, using a JSON type definition instead of a JSON schema, inspired by this blog post I came accross. The prompt with the type definition is shorter, produces better results and is more human-readable.\nYou are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n\n{\n    \"date\": string,\n    \"subject\": string,\n    \"sender\": {\n        \"name\": string | null,\n        \"email\": string,\n        \"phone_number\": string | null,\n        \"role\": string | null,\n        \"organization\": string | null\n    },\n    \"recipients\": {\n        \"name\": string | null,\n        \"email\": string,\n        \"phone_number\": string | null,\n        \"role\": string | null,\n        \"organization\": string | null,\n        \"to\": enum([\"to\", \"cc\", \"bcc\"])\n    }[]\n}\n\n\nextract_information_with_llm: Function that extracts information using an LLM\ndef extract_information_with_llm(\n    raw_email: str, *, system_prompt: str\n) -&gt; EmailInformation:\n    \"\"\"Extracts structured information from a raw email using an LLM.\n\n    Uses chat completion API to parse email content into structured format.\n    Enforces output schema validation using EmailInformation model specification.\n\n    Args:\n        raw_email: Raw email text including headers and body.\n        system_prompt: System prompt for the LLM that defines the extraction task.\n\n    Returns:\n        Structured object containing the extracted information, validated against the EmailInformation schema.\n    \"\"\"\n    response_format = {\n        \"type\": \"json_object\",\n        \"schema\": EmailInformation.model_json_schema(),\n    }\n    output = llm.create_chat_completion_openai_v1(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\"role\": \"user\", \"content\": raw_email},\n        ],\n        response_format=response_format,\n        temperature=0.3,\n    )\n    extracted_information = EmailInformation.model_validate_json(\n        output.choices[0].message.content\n    )\n    return extracted_information\n\n\n\n\n\n\n\nUsing this method, we extract the following information from the sample test email:\nSample extracted information with llm zero-shot prompt:\n{\n    \"date\": \"Thu, 10 May 2001 05:08:00 -0700 (PDT)\",\n    \"subject\": \"FP&L\",\n    \"sender\": {\n        \"name\": \"Kam Keiser\",\n        \"email\": \"kam.keiser@enron.com\",\n        \"phone_number\": null,\n        \"role\": \"X-From\",\n        \"organization\": \"X-Origin: Neal-S\"\n    },\n    \"recipients\": [\n        {\n            \"name\": \"Sabra L Dinari\",\n            \"email\": \"sabra.dinari@enron.com\",\n            \"phone_number\": null,\n            \"role\": \"X-To\",\n            \"organization\": \"X-Origin: Neal-S\",\n            \"type\": \"to\"\n        },\n        {\n            \"name\": \"Yuan Tian\",\n            \"email\": \"yuan.tian@enron.com\",\n            \"phone_number\": null,\n            \"role\": \"X-cc\",\n            \"organization\": \"X-Origin: Neal-S\",\n            \"type\": \"cc\"\n        },\n        {\n            \"name\": \"Scott Neal\",\n            \"email\": \"scott.neal@enron.com\",\n            \"phone_number\": null,\n            \"role\": \"X-cc\",\n            \"organization\": \"X-Origin: Neal-S\",\n            \"type\": \"cc\"\n        }\n    ]\n}\"\nWe also compute the average score of this approach on the test set:\n64.55%\n\n\n\n\n\nLLM Zero-Shot Prompt Score\n\n\n\n\nWe can see that this approach performs much worse than the previous one, most likely due to mismatch in expected formats. For example, we expected the date to be of the form 11.07.2001 instead of 11/07/2001, 11 Jul 2001 or Wed, 11 Jul 2001.\nTo fix that, we will provide an example of information extraction in the system prompt in order to better guide the LLM."
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html#second-approach---llm-few-shot-extraction-with-json-schema",
    "href": "posts/llm-email-information-extaction/index.html#second-approach---llm-few-shot-extraction-with-json-schema",
    "title": "Using an LLM for information extraction",
    "section": "Second approach - LLM few-shot extraction with JSON schema",
    "text": "Second approach - LLM few-shot extraction with JSON schema\nTo improve performance, we move to a few-shot approach. Given context length and performance constraints, we use one example from the training set in the system prompt, making it a one-shot prompt approach.\nWe evaluate the impact of each example from the training set on the remaining examples in the training set to determine which one best improves the information extraction.\n\n\nCode\nscore_improvements = []\n\nfor i in trange(len(train_set), desc=\"Example\"):\n    example = train_set[i]\n    train_set_without_example = train_set[:i] + train_set[i + 1 :]\n\n    # We first compute the similarity with the zero-shot system prompt\n    scores_zero_shot = evaluate_extraction(\n        partial(extract_information_with_llm, system_prompt=system_prompt),\n        train_set_without_example,\n    )\n    mean_score_zero_shot = np.mean(scores_zero_shot).item()\n\n    # We then compute the similarity with the one-shot (with example) system prompt\n    system_prompt_with_example = (\n        system_prompt\n        + f\"\"\"\nUse the following example of raw email and extracted information as reference:\n\n# Raw email\n\n{example[\"raw_email\"]}\n\n# Extracted information\n\n{example[\"extracted_information\"].model_dump_json(indent=2)}\n\"\"\"\n    )\n    scores_one_shot = evaluate_extraction(\n        partial(extract_information_with_llm, system_prompt=system_prompt_with_example),\n        train_set_without_example,\n    )\n    mean_score_one_shot = np.mean(scores_one_shot).item()\n    # We then compute the difference in similarity\n    score_improvement = mean_score_one_shot - mean_score_zero_shot\n    score_improvements.append(\n        (\n            score_improvement,\n            system_prompt_with_example,\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM One-Shot Prompt Score Improvement\n\n\n\n\nThe best score improvement we can obtain using the one-shot prompt and out small train set is: 31.45\nThe system prompt and, by extension, example corresponding to this improvement are:\nYou are a helpful assistant that extract information from a user provided email in JSON format that adheres to the following schema:\n\n{\n    \"date\": string,\n    \"subject\": string,\n    \"sender\": {\n        \"name\": string | null,\n        \"email\": string,\n        \"phone_number\": string | null,\n        \"role\": string | null,\n        \"organization\": string | null\n    },\n    \"recipients\": {\n        \"name\": string | null,\n        \"email\": string,\n        \"phone_number\": string | null,\n        \"role\": string | null,\n        \"organization\": string | null,\n        \"to\": enum([\"to\", \"cc\", \"bcc\"])\n    }[]\n}\n\nUse the following example of raw email and extracted information as reference:\n\n# Raw email\n\nMessage-ID: &lt;7555575.1075857946274.JavaMail.evans@thyme&gt;\nDate: Mon, 30 Apr 2001 11:17:00 -0700 (PDT)\nFrom: cathy@pira.com\nTo: greg@pira.com\nSubject: PIRA's Special Release Gas Flash: Strong Supply Growth Indicated\n 04/30/01\nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nX-From: \"Cathy Lopez\" &lt;cathy@pira.com&gt;\nX-To: \"PIRA Natural Gas Retainer Client\" &lt;greg@pira.com&gt;\nX-cc:\nX-bcc:\nX-Folder: \\Lawrence_May_Jun2001_1\\Notes Folders\\Notes inbox\nX-Origin: May-L\nX-FileName: lmay2.nsf\n\nAttached is a PIRA Special Release entitled \"Gas Flash: Strong Supply Growth\nIndicated.\"\n\nIf you have any questions regarding the report's content, please contact:\nGreg Shuttlesworth (email: greg@pira.com), Tom Howard (email:\ntazh@pira.com), Richard Redash (email: Rich@pira.com), Nobu Tarui (email:\nnobuo@pira.com) or Jane Hsu (email: jane@pira.com), at (212) 686-6808.\n\nContact John Graziano regarding PIRA report distribution and address changes\nat (212) 686-6808, email: support@pira.com.\n\nNOTE: Circulation of the \"Gas Flash Weekly\" outside a Client's licensed\ndistribution area is strictly prohibited. Clients that are unsure of their\nlicensed distribution or require an extension of their current license\nshould contact their PIRA sales representative, or email to sales@pira.com.\n\nPIRA Energy Group\n\n\n\n\n - agaspec043001.pdf\n\n# Extracted information\n\n{\n  \"date\": \"30.04.2001\",\n  \"subject\": \"PIRA's Special Release Gas Flash: Strong Supply Growth Indicated 04/30/01\",\n  \"sender\": {\n    \"name\": \"Cathy Lopez\",\n    \"email\": \"cathy@pira.com\",\n    \"phone_number\": null,\n    \"role\": null,\n    \"organization\": \"PIRA Energy Group\"\n  },\n  \"recipients\": [\n    {\n      \"name\": \"Greg Shuttlesworth\",\n      \"email\": \"greg@pira.com\",\n      \"phone_number\": null,\n      \"role\": null,\n      \"organization\": \"PIRA Energy Group\",\n      \"type\": \"to\"\n    }\n  ]\n}\n\n\n\n\n\nUsing this method, we extract the following information from the sample test email:\nSample extracted information with llm one-shot prompt:\n{\n    \"date\": \"10.05.2001\",\n    \"subject\": \"FP&L\",\n    \"sender\": {\n        \"name\": \"Kam Keiser\",\n        \"email\": \"kam.keiser@enron.com\",\n        \"phone_number\": null,\n        \"role\": null,\n        \"organization\": \"Enron\"\n    },\n    \"recipients\": [\n        {\n            \"name\": \"Sabra L Dinari\",\n            \"email\": \"sabra.dinari@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": \"Enron\",\n            \"type\": \"to\"\n        },\n        {\n            \"name\": \"Yuan Tian\",\n            \"email\": \"yuan.tian@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": \"Enron\",\n            \"type\": \"cc\"\n        },\n        {\n            \"name\": \"Scott Neal\",\n            \"email\": \"scott.neal@enron.com\",\n            \"phone_number\": null,\n            \"role\": null,\n            \"organization\": \"Enron\",\n            \"type\": \"cc\"\n        }\n    ]\n}\"\nThe average score of this second approach on the test set is:\n87.27%\n\n\n\n\n\nLLM One-Shot Prompt Score"
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html#comparison-of-the-3-approaches",
    "href": "posts/llm-email-information-extaction/index.html#comparison-of-the-3-approaches",
    "title": "Using an LLM for information extraction",
    "section": "Comparison of the 3 approaches",
    "text": "Comparison of the 3 approaches\nWhen comparing the scores of the three approaches, it is clear that both the baseline parser and the few-shot approach outperform the zero-shot approach. The few-shot approach is more flexible and generalizes better, which could prove beneficial when scaling to larger datasets.\n\n\n\n\n\nLLM One-Shot Prompt Score"
  },
  {
    "objectID": "posts/llm-email-information-extaction/index.html#footnotes",
    "href": "posts/llm-email-information-extaction/index.html#footnotes",
    "title": "Using an LLM for information extraction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEnron Corporation was an American energy, commodities, and services that went bankrupt in 2001 after ascandal involving a corporate fraud case. The Enron Email Dataset, containing 500,000 internal emails, provides valuable insights into the company’s operations and is widely used for research in text mining and network analysis.↩︎"
  },
  {
    "objectID": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html",
    "href": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html",
    "title": "When ECR lifecycle policies are not enough…",
    "section": "",
    "text": "In this second post we will talk about something a bit different from last time. we will be using a tool to extend and take full advantage of ECR Lifecycle Policies to prune more images than is possible with just lifecycle policies.\nWe will be using AWS, a Kubernetes Cluster (self-hosted or EKS), as well as a Continuous Integration system, or CI for short, to build and push images continously to ECR.\nLet’s assume that we are continously building and pushing images to ECR after each push to the master branch. If we just keep doing that we will eventually hit the maximum number of images in a given repository and receive an error message for every subsequent push attempt.\nIn most cases, especially at the beginning of a project, there won’t be a problem because the maximum number of images per repository is quite high (the limit used to be 1000 but was recently increased to 10000).\nFor all other cases it is recommended to use ECR Lifecycle Policies to, at the very least, remove untagged images and prune very old images. But, once we start using them we quickly realize that the allowed rules are quite limited. For example we cannot currently remove all images with a certain tag prefix older than a certain number of days while keeping the last N even if they are too old.\nHere are some examples of rules that cannot be created using them:\nIn this post we will focus on solving the problem from the last example.\nTo do so we could either create our own service/tool to directly prune images and avoid using ECR Lifecycle Policies or we could complement it with a small service/tool that adds a given tag to certain images.\nI went with the later and wrote a small service called kube-ecr-tagger that runs inside the cluster and tags images for us.\nBefore presenting it, let’s have a look into ECR Lifecycle Policies a bit more in detail."
  },
  {
    "objectID": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#ecr-lifecycle-policies",
    "href": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#ecr-lifecycle-policies",
    "title": "When ECR lifecycle policies are not enough…",
    "section": "ECR Lifecycle Policies",
    "text": "ECR Lifecycle Policies\nECR lifecycle policies are a part of ECR that enables us to specify the lifecycle management of images in a repository. A lifecycle policy, such as the following, is a set of one or more rules, where each rule defines an action for ECR. The actions apply to images that contain tags prefixed with the given strings. This allows the automation of cleaning up unused images, for example expiring images based on age or count:\n{\n    \"rules\": [\n        {\n            \"rulePriority\": 1,\n            \"description\": \"Expire images older than 14 days\",\n            \"selection\": {\n                \"tagStatus\": \"untagged\",\n                \"countType\": \"sinceImagePushed\",\n                \"countUnit\": \"days\",\n                \"countNumber\": 14\n            },\n            \"action\": {\n                \"type\": \"expire\"\n            }\n        }\n    ]\n}\nThe allowed rules and actions are limited. For example, you cannot do any of the following:\n\nMatch the same image with multiple rules ( this could be used to add exceptions to a rule ). If a rule matches an image it cannot be matched by another rule with a lower priority\nExpire images both by count and by age\nChoose to keep images that match a rule instead of expiring them\nMatch images with an exact tag, only tag prefixes are allowed\n\nTo avoid some of these limitations and to solve our problem from the previous section I developed a simple tool called kube-ecr-tagger."
  },
  {
    "objectID": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#kube-ecr-tagger",
    "href": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#kube-ecr-tagger",
    "title": "When ECR lifecycle policies are not enough…",
    "section": "kube-ecr-tagger",
    "text": "kube-ecr-tagger\nkube-ecr-tagger runs inside the cluster and tags the used ECR images in a given namespace, or in all namespaces, with either a given tag or a tag created by appending a unix timestamp to the passed tag prefix.\nIf a tag is passed then there will only be one such tag in each repository. If a tag prefix is passed instead then there will be multiple tags in each repository with the same prefix.\nLet’s use a concrete example. Let’s assume that we use semantic versioning to tag our images in CI and that we are currently at version 1.0.X and that not all images that are pushed will be used in production. Perhaps some of them will fail in the acceptance/staging phase or be flagged for security issues if image scanning is activated.\nWe want to keep the last 100 images that are deployed in production and at the same time remove images whose tag starts with ‘1.0’ and that are older than 30 days. For that we could the following lifecycle policy:\n{\n    \"rules\": [\n        {\n            \"rulePriority\": 10,\n            \"description\": \"Keep last 100 images with tag prefix 'production' \",\n            \"selection\": {\n                \"tagStatus\": \"tagged\",\n                \"tagPrefixList\": [\"production\"],\n                \"countType\": \"imageCountMoreThan\",\n                \"countNumber\": 100\n            },\n            \"action\": { \"type\": \"expire\" }\n        },\n        {\n            \"rulePriority\": 20,\n            \"description\": \"Remove images with tag prefix '1.0' that are older than 30 days\",\n            \"selection\": {\n                \"tagStatus\": \"tagged\",\n                \"tagPrefixList\": [\"1.0\"],\n                \"countType\": \"sinceImagePushed\",\n                \"countNumber\": 30,\n                \"countUnit\": \"days\"\n            },\n            \"action\": { \"type\": \"expire\" }\n        }\n    ]\n}\nThen, in order to tag the images in production, which we assume for the sake of simplicity are deployed in the prod namespace, we will deploy kube-ecr-tagger as a Deployment in the kube-system namespace with the following example manifest:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: kube-ecr-tagger\n   namespace: kube-system\nspec:\n   template:\n      spec:\n        containers:\n         - name: kube-ecr-tagger\n           image: anesbenmerzoug/kube-ecr-tagger:latest \n           command:\n           - kube-ecr-tagger\n           args:\n           - --namespace=prod\n           - --tag-prefix=production\nWe should not forget to add a service account, if we’re using IAM roles for service accounts, or the right annotation, if we’re using kiam or kube2iam, with the right IAM permissions:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:DescribeImages\",\n                \"ecr:BatchGetImage\",\n                \"ecr:PutImage\",\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nExample manifests can be found in the kube-ecr-tagger repository under the manifests folder.\nOnce deployed, it will then check the ECR images of all deployed containers in the prod namespace and add tags with the prefix production to each image, if one does not exist already."
  },
  {
    "objectID": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#conclusion",
    "href": "posts/when-ecr-lifecycle-policies-are-not-enough/index.html#conclusion",
    "title": "When ECR lifecycle policies are not enough…",
    "section": "Conclusion",
    "text": "Conclusion\nWe have seen in this post that by combining a small tool with ECR Lifecycle Policies we can achieve more complicated rules than with plain lifecycle policies.\nOf course, kube-ecr-tagger is limited in what it can do but one can already have an idea of what can be achieved.\nIf you want to try kube-ecr-tagger you can simply use the built container images from this repository on Dockerhub.\nI hope that you have learned at a thing or two from this post. If there are any mistakes or if you have questions please do not hesitate to reach out to me."
  },
  {
    "objectID": "posts/control-series/index.html",
    "href": "posts/control-series/index.html",
    "title": "Series: Control",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nAnes Benmerzoug\n\n\n\n\n\n\nNo matching items"
  }
]